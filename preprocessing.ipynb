{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":116111,"databundleVersionId":13910868,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Hotel Property Value â€” Preprocessing & EDA\n\n**Author(s):** Gradient Gang\n\n**Purpose:** EDA, cleaning, feature engineering, and preprocessing pipelines.","metadata":{}},{"cell_type":"code","source":"# === Basic Imports ===\nimport os, random, gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\n\n\n# === File Paths ===\nTRAIN_PATH = \"/kaggle/input/Hotel-Property-Value-Dataset/train.csv\"\nTEST_PATH  = \"/kaggle/input/Hotel-Property-Value-Dataset/test.csv\"\nSAMPLE_PATH = \"/kaggle/input/Hotel-Property-Value-Dataset/sample_submission.csv\"\n\n# === Load Data ===\ntrain = pd.read_csv(TRAIN_PATH)\ntest  = pd.read_csv(TEST_PATH)\nsample = pd.read_csv(SAMPLE_PATH)\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# === Check for Duplicates ===\ntrain.drop_duplicates(inplace=True)\ntest.drop_duplicates(inplace=True)\n\nprint(\"Train shape:\", train.shape)\nprint(\"Test shape:\", test.shape)\n\n\n# === Missing Values ===\nnull_value_percentages=(train.isna().sum()/train.shape[0])*100\nprint(\"\\nNull value percentages (training):\\n\", null_value_percentages)\n\ncolumns_with_null_values = null_value_percentages[null_value_percentages>0]\nprint(\"\\nColumns with null values (training):\\n\",columns_with_null_values)\n\n\nnull_value_percentages_test=(test.isna().sum()/test.shape[0])*100\nprint(\"\\nNull value percentages (test):\\n\", null_value_percentages_test)\n\ncolumns_with_null_values_test = null_value_percentages_test[null_value_percentages_test>0]\nprint(\"\\nColumns with null values (test):\\n\",columns_with_null_values_test)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here we have divided the columns based on the percentages of null values into the following categories:\n* Columns with very less null values (0-5%)\n* Columns with moderate amount of null values (5-50%)\n* Columns with a lot of null values (>50%)","metadata":{}},{"cell_type":"markdown","source":"### Null Value Stuff","metadata":{}},{"cell_type":"code","source":"#=== Dropping Columns with lot of null values ===\ncolumns_to_drop=null_value_percentages[null_value_percentages>50]\n\ncolumns_to_drop=columns_to_drop.keys()\nprint(\"training data:\",columns_to_drop)\n\ntrain.drop(columns=columns_to_drop,inplace=True)\ntrain.drop_duplicates(inplace=True)\n\n#-----------------------------------------------------------\ncolumns_to_drop_test=null_value_percentages_test[null_value_percentages_test>50]\n\ncolumns_to_drop_test=columns_to_drop_test.keys()\nprint(\"testing data:\",columns_to_drop_test)\n\ntest.drop(columns=columns_to_drop_test,inplace=True)\ntest.drop_duplicates(inplace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#=== Checking Columns with moderate amount of null values ===\nmoderate_null_cols = null_value_percentages[(null_value_percentages >= 5) & (null_value_percentages <= 50)].index\nmoderate_null_cols_test = null_value_percentages_test[(null_value_percentages_test >= 5) & (null_value_percentages_test <= 50)].index\n\nprint(\"training data:\",moderate_null_cols)\nprint(\"testing data:\",moderate_null_cols_test)\n\nprint(\"\\n\")\n\nfor column in moderate_null_cols:\n    if train[column].dtype == 'object':\n        print(f\"CATEGORICAL: {column}\")\n        print(train[column].value_counts(dropna=False))\n    else:\n        missing_count = train[column].isna().sum()\n        print(f\"NUMERIC: {column}\")\n        print(train[column].describe())\n        print(f\"Missing values: {missing_count}\")\n        \nprint(\"-\" * 50)\nfor column in moderate_null_cols_test:\n    if test[column].dtype == 'object':\n        print(f\"CATEGORICAL: {column}\")\n        print(test[column].value_counts(dropna=False))\n    else:\n        missing_count = test[column].isna().sum()\n        print(f\"NUMERIC: {column}\")\n        print(test[column].describe())\n        print(f\"Missing values: {missing_count}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#=== Imputing Columns with moderate amount of null values ===\n\n# Columns where null represents absence\ncategorical_absence_map = {\n    'LoungeQuality': 'NoLounge',\n    'ParkingType': 'NoParking',\n    'ParkingFinish': 'NoParking',\n    'ParkingQuality': 'NoParking',\n    'ParkingCondition': 'NoParking'\n}\n\n# Seed for reproducibility\nnp.random.seed(42)\n\n# ===== Imputation =====\nfor col in moderate_null_cols:\n    # Add missing flag\n    train[f\"{col}_was_missing\"] = train[col].isna().astype(int)\n    test[f\"{col}_was_missing\"] = test[col].isna().astype(int)\n\n    # Categorical columns with \"absence\" meaning\n    if col in categorical_absence_map:\n        train[col] = train[col].fillna(categorical_absence_map[col])\n        if col in test.columns:\n            test[col] = test[col].fillna(categorical_absence_map[col])\n\n    # Other categorical columns (randomly assign observed values)\n    elif train[col].dtype == 'object':\n        possible_values = train[col].dropna().unique()\n        null_mask = train[col].isna()\n        n_missing = null_mask.sum()\n        train.loc[null_mask, col] = np.random.choice(possible_values, size=n_missing, replace=True)\n\n        if col in test.columns:\n            possible_values_test = test[col].dropna().unique()\n            null_mask_test = test[col].isna()\n            n_missing_test = null_mask_test.sum()\n            test.loc[null_mask_test, col] = np.random.choice(possible_values_test, size=n_missing_test, replace=True)\n\n    # Numeric columns (sample from actual observed values)\n    else:\n        observed_values = train[col].dropna().values\n        null_mask = train[col].isna()\n        n_missing = null_mask.sum()\n        train.loc[null_mask, col] = np.random.choice(observed_values, size=n_missing, replace=True)\n\n        if col in test.columns:\n            observed_values_test = test[col].dropna().values\n            null_mask_test = test[col].isna()\n            n_missing_test = null_mask_test.sum()\n            test.loc[null_mask_test, col] = np.random.choice(observed_values_test, size=n_missing_test, replace=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#=== Verification ===\n\nfor column in moderate_null_cols:\n    if train[column].dtype == 'object':\n        print(f\"CATEGORICAL: {column}\")\n        print(train[column].value_counts(dropna=False))\n    else:\n        missing_count = train[column].isna().sum()\n        print(f\"NUMERIC: {column}\")\n        print(train[column].describe())\n        print(f\"Missing values: {missing_count}\")\n        \nprint(\"-\" * 50)\nfor column in moderate_null_cols_test:\n    if test[column].dtype == 'object':\n        print(f\"CATEGORICAL: {column}\")\n        print(test[column].value_counts(dropna=False))\n    else:\n        missing_count = test[column].isna().sum()\n        print(f\"NUMERIC: {column}\")\n        print(test[column].describe())\n        print(f\"Missing values: {missing_count}\")\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#=== Checking Columns with less amount of null values ===\nless_null_cols = null_value_percentages[(null_value_percentages > 0) & (null_value_percentages < 5)].index\nless_null_cols_test = null_value_percentages_test[(null_value_percentages_test > 0) & (null_value_percentages_test < 5)].index\n\nprint(\"Training data columns with low nulls:\", less_null_cols)\nprint(\"Testing data columns with low nulls:\", less_null_cols_test)\nprint(\"=\"*80)\n\n# Function to inspect a column safely\ndef inspect_column(df, col):\n    missing_count = df[col].isna().sum()\n    missing_pct = missing_count / len(df) * 100\n    \n    print(f\"Column: {col}\")\n    print(f\"Type: {'Categorical' if df[col].dtype=='object' else 'Numeric'}\")\n    print(f\"Missing: {missing_count} ({missing_pct:.3f}%)\")\n    \n    if df[col].dtype == 'object':\n        # Include NaN explicitly\n        print(\"Value counts (including NaN):\")\n        print(df[col].value_counts(dropna=False))\n    else:\n        print(\"Descriptive stats (numeric):\")\n        print(df[col].describe())\n    print(\"-\"*50)\n\n# Inspect training data\nprint(\"TRAINING DATA:\")\nfor col in less_null_cols:\n    inspect_column(train, col)\n\n# Inspect testing data\nprint(\"TESTING DATA:\")\nfor col in less_null_cols_test:\n    inspect_column(test, col)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nnp.random.seed(42)\n\n# --- Columns ---\nless_null_cols = null_value_percentages[(null_value_percentages > 0) & (null_value_percentages < 5)].index\nless_null_cols_test = null_value_percentages_test[(null_value_percentages_test > 0) & (null_value_percentages_test < 5)].index\n\n# Basement related columns\nbasement_cols = [\n    'BasementHeight', 'BasementCondition', 'BasementExposure',\n    'BasementFacilityType1', 'BasementFacilityType2'\n]\n\n# Other columns\nnumeric_cols = ['FacadeArea']\ncategorical_cols = ['ElectricalSystem']\n\n# --- Function to impute one dataset ---\ndef impute_low_nulls(df, is_test=False):\n    # Add missing flags\n    for col in less_null_cols:\n        if col in df.columns:\n            df[f\"{col}_was_missing\"] = df[col].isna().astype(int)\n    \n    # --- Basement columns ---\n    for idx, row in df.iterrows():\n        if all(col in df.columns for col in basement_cols):\n            if row[basement_cols].isna().all():\n                df.loc[idx, basement_cols] = \"NoBasement\"\n            else:\n                for col in basement_cols:\n                    if pd.isna(row[col]):\n                        possible_values = df[col].dropna().unique()\n                        df.loc[idx, col] = np.random.choice(possible_values)\n\n    # --- Numeric columns ---\n    for col in numeric_cols:\n        if col in df.columns:\n            null_mask = df[col].isna()\n            n_missing = null_mask.sum()\n            if n_missing > 0:\n                observed_vals = df[col].dropna().values\n                df.loc[null_mask, col] = np.random.choice(observed_vals, size=n_missing, replace=True)\n\n    # --- Other categorical columns ---\n    for col in categorical_cols:\n        if col in df.columns:\n            null_mask = df[col].isna()\n            n_missing = null_mask.sum()\n            if n_missing > 0:\n                observed_vals = df[col].dropna().unique()\n                df.loc[null_mask, col] = np.random.choice(observed_vals, size=n_missing, replace=True)\n\n# --- Apply to train and test ---\nimpute_low_nulls(train)\nimpute_low_nulls(test)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#=== Checking Columns with less amount of null values ===\nprint(\"Training data columns with low nulls:\", less_null_cols)\nprint(\"Testing data columns with low nulls:\", less_null_cols_test)\nprint(\"=\"*80)\n\n# Function to inspect a column safely\ndef inspect_column(df, col):\n    missing_count = df[col].isna().sum()\n    missing_pct = missing_count / len(df) * 100\n    \n    print(f\"Column: {col}\")\n    print(f\"Type: {'Categorical' if df[col].dtype=='object' else 'Numeric'}\")\n    print(f\"Missing: {missing_count} ({missing_pct:.3f}%)\")\n    \n    if df[col].dtype == 'object':\n        # Include NaN explicitly\n        print(\"Value counts (including NaN):\")\n        print(df[col].value_counts(dropna=False))\n    else:\n        print(\"Descriptive stats (numeric):\")\n        print(df[col].describe())\n    print(\"-\"*50)\n\n# Inspect training data\nprint(\"TRAINING DATA:\")\nfor col in less_null_cols:\n    inspect_column(train, col)\n\n# Inspect testing data\nprint(\"TESTING DATA:\")\nfor col in less_null_cols_test:\n    inspect_column(test, col)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Outlier detection","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n# Identify numeric columns\nnumeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\nnumeric_cols_test = test.select_dtypes(include=[np.number]).columns.tolist()\n\nprint(\"Numeric columns (train):\", numeric_cols)\nprint(\"Numeric columns (test):\", numeric_cols_test)\n\n# Class to cap outliers using IQR\nclass OutlierHandler:\n    def __init__(self, col):\n        q1 = col.quantile(0.25)\n        q3 = col.quantile(0.75)\n        iqr = q3 - q1\n        self.lower_whisker = q1 - 1.5 * iqr\n        self.upper_whisker = q3 + 1.5 * iqr\n\n    def cap(self, value):\n        if value < self.lower_whisker:\n            return self.lower_whisker\n        elif value > self.upper_whisker:\n            return self.upper_whisker\n        else:\n            return value\n\n# Apply outlier capping and plot for training data\nfor col in numeric_cols:\n    handler = OutlierHandler(train[col])\n    train[col] = train[col].apply(handler.cap)\n\n #   plt.figure(figsize=(6, 3))\n #   sns.boxplot(x=train[col])\n#    plt.title(f\"{col} distribution (train)\")\n #   plt.show()\n\n# Apply outlier capping for test data\nfor col in numeric_cols_test:\n    handler = OutlierHandler(test[col])\n    test[col] = test[col].apply(handler.cap)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Standardisation","metadata":{}},{"cell_type":"code","source":"# Identify numeric columns in train\nnumeric_cols = train.select_dtypes(include=[np.number]).columns.tolist()\n\n# Find numeric columns that exist in both train and test\ncommon_numeric_cols = [col for col in numeric_cols if col in test.columns]\n\n# Initialize scaler\nscaler = StandardScaler()\n\n# Standardize train\ntrain[common_numeric_cols] = scaler.fit_transform(train[common_numeric_cols])\n\n# Standardize test using the same scaler\ntest[common_numeric_cols] = scaler.transform(test[common_numeric_cols])\n\n# Optional: check the new statistics\n'''\ntarget_col = \"RoadAccessLength\"  # replace if your target column has a different name\n\nfor col in common_numeric_cols:\n    if col == target_col:\n        continue\n    plt.figure(figsize=(6, 4))\n    sns.scatterplot(x=train[col], y=train[target_col])\n    plt.title(f\"Scatter plot: {col} vs {target_col}\")\n    plt.xlabel(col)\n    plt.ylabel(target_col)\n    plt.show()\n'''\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)\n\n# Drop all helper \"_was_missing\" columns\ntrain_clean = train[[col for col in train.columns if not col.endswith('_was_missing')]]\ntest_clean = test[[col for col in test.columns if not col.endswith('_was_missing')]]\n\n# Identify numeric and categorical columns again\nnumeric_cols = train_clean.select_dtypes(include=[np.number]).columns.tolist()\ncategorical_cols = train_clean.select_dtypes(include=['object']).columns.tolist()\n\n# Plot histograms for numeric columns\nplt.figure(figsize=(15, len(numeric_cols)*3))\nfor i, col in enumerate(numeric_cols):\n    plt.subplot(len(numeric_cols), 1, i+1)\n    sns.histplot(train_clean[col], kde=True, bins=30)\n    plt.title(f\"Histogram of {col}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Identify categorical columns\ncategorical_cols = train_clean.select_dtypes(include=['object']).columns.tolist()\n\n# Plot boxplots of categorical features vs target variable\nplt.figure(figsize=(15, len(categorical_cols)*3))\nfor i, col in enumerate(categorical_cols):\n    plt.subplot(len(categorical_cols), 1, i+1)\n    sns.boxplot(x=train_clean[col], y=train_clean['HotelValue'])\n    plt.title(f\"Boxplot of HotelValue vs {col}\")\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Saving the processed Datasets","metadata":{}},{"cell_type":"code","source":"# Save train and test to CSV\ntrain_clean.to_csv(\"train_processed.csv\", index=False)\ntest_clean.to_csv(\"test_processed.csv\", index=False)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}